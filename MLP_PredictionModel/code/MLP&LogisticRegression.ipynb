{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "# Saved in the d2l package for later use\n",
    "class Timer(object):\n",
    "    \"\"\"Record multiple running times.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        # Start the timer\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        # Stop the timer and record the time in a list\n",
    "        self.times.append(time.time() - self.start_time)\n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self):\n",
    "        # Return the average time\n",
    "        return sum(self.times)/len(self.times)\n",
    "\n",
    "    def sum(self):\n",
    "        # Return the sum of time\n",
    "        return sum(self.times)\n",
    "\n",
    "    def cumsum(self):\n",
    "        # Return the accumuated times\n",
    "        return np.array(self.times).cumsum().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import d2l\n",
    "import pandas as pd\n",
    "import mxnet\n",
    "import numpy\n",
    "from mxnet import np\n",
    "from mxnet import autograd, init, gluon\n",
    "from mxnet import ndarray as nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy for a single prediction\n",
    "def accuracy(output, label):\n",
    "    return nd.mean(output.argmax(axis = 1) == label).asscalar()\n",
    "\n",
    "# Accuracy for the whole test data iteretor\n",
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = 0.\n",
    "    for data, label in data_iterator:\n",
    "        output = net(data)\n",
    "        #print(output)\n",
    "        acc += accuracy(output, label)\n",
    "        #break\n",
    "    return acc/len(data_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "accident = pd.read_csv('../data/US_Accidents_2019_Segment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(953630, 13)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accident = accident.dropna(subset = ['Severity']) # Make sure each the label has no nan values\n",
    "accident.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Wind_Speed(mph)', 'Precipitation(in)', 'Temperature(F)',\n",
       "       'Wind_Chill(F)', 'Wind_Direction', 'Humidity(%)', 'Pressure(in)',\n",
       "       'Visibility(mi)', 'Severity', 'Distance(mi)', 'Side',\n",
       "       'Weather_Condition', 'Sunrise_Sunset'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accident.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample some data from each type of label\n",
    "df2 = accident[accident.Severity == 2].sample(30000)\n",
    "df3 = accident[accident.Severity == 3].sample(30000)\n",
    "df4 = accident[accident.Severity == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put each sampled data together\n",
    "tempData = pd.concat([df2,df3,df4])\n",
    "# make the label strat from 0\n",
    "tempData.Severity = tempData.Severity -2\n",
    "# make a random array\n",
    "sampler = numpy.random.permutation(tempData.shape[0])\n",
    "# randomly sort the data\n",
    "tempData = tempData.take(sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = tempData.dtypes[tempData.dtypes == 'float64'].index\n",
    "tempData[numeric_features] = tempData[numeric_features].apply(lambda x: (x - x.mean()/x.std()))\n",
    "#tempData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using one-hot encoding to express nominal data\n",
    "dummyData = pd.get_dummies(tempData.dropna())\n",
    "dummyData = dummyData.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63797, 104)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummyData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using 7:3 Train-Test Data to Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = numpy.random.permutation(dummyData.shape[0])\n",
    "# randomly take 70% of data to be the train data and 30% to be the test data\n",
    "train_data = dummyData.take(sampler[0:int(len(sampler)*0.7)])\n",
    "test_data = dummyData.take(sampler[int(len(sampler)*0.7):])\n",
    "\n",
    "train_features = train_data.drop(columns = 'Severity')\n",
    "test_features = test_data.drop(columns ='Severity')\n",
    "\n",
    "train_labels = train_data['Severity']\n",
    "test_labels = test_data['Severity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "#turn the dataset to ndarray, and add it to data loader for later use\n",
    "train_features_array = nd.array(train_features.values, dtype = 'float32')\n",
    "test_features_array = nd.array(test_features.values, dtype = 'float32')\n",
    "\n",
    "train_labels_array = nd.array(train_labels.values, dtype = 'float32')\n",
    "test_labels_array = nd.array(test_labels.values, dtype = 'float32')\n",
    "\n",
    "train_iter = d2l.load_array((train_features_array, train_labels_array), batch_size)\n",
    "test_iter = d2l.load_array((test_features_array, test_labels_array), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_data, test_data, num_epoch,batch_size):\n",
    "    for epoch in range(num_epoch):\n",
    "        train_loss = 0.\n",
    "        train_acc = 0.\n",
    "        # iterate the training data to network\n",
    "        for data, label in train_data:\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                # compute the loss \n",
    "                loss = softmax_cross_entropy(output, label)\n",
    "            # calculate the gradient of loss\n",
    "            loss.backward()\n",
    "            # used adam to upgrate the parameters\n",
    "            trainer.step(batch_size = 32)\n",
    "            # calculate the loss and accuracy of the module\n",
    "            train_loss += nd.mean(loss).asscalar()\n",
    "            train_acc += accuracy(output, label)\n",
    "        # calculate the accuracy of test data\n",
    "        test_acc = evaluate_accuracy(test_data, net)\n",
    "        # print the result every 5 epochs\n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch: %d, Loss: %f, Train acc: %f, Test acc: %f' % \n",
    "                  (epoch, train_loss/len(train_data), train_acc/len(train_data), test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1.088227, Train acc: 0.373768, Test acc: 0.466485\n",
      "Epoch: 5, Loss: 0.934964, Train acc: 0.539424, Test acc: 0.542321\n",
      "Epoch: 10, Loss: 0.896186, Train acc: 0.565953, Test acc: 0.571512\n",
      "Epoch: 15, Loss: 0.875305, Train acc: 0.582327, Test acc: 0.589828\n",
      "Epoch: 20, Loss: 0.864193, Train acc: 0.585355, Test acc: 0.594765\n",
      "Epoch: 25, Loss: 0.858195, Train acc: 0.589232, Test acc: 0.602164\n",
      "Epoch: 30, Loss: 0.841868, Train acc: 0.594243, Test acc: 0.608973\n",
      "Epoch: 35, Loss: 0.845170, Train acc: 0.592568, Test acc: 0.609122\n",
      "Epoch: 40, Loss: 0.838805, Train acc: 0.595537, Test acc: 0.616132\n",
      "Epoch: 45, Loss: 0.839641, Train acc: 0.595606, Test acc: 0.600887\n",
      "Epoch: 50, Loss: 0.839284, Train acc: 0.594997, Test acc: 0.600232\n",
      "Epoch: 55, Loss: 0.837469, Train acc: 0.591077, Test acc: 0.587477\n",
      "Epoch: 60, Loss: 0.828360, Train acc: 0.598037, Test acc: 0.478410\n",
      "Epoch: 65, Loss: 0.830244, Train acc: 0.598680, Test acc: 0.611515\n",
      "Epoch: 70, Loss: 0.837616, Train acc: 0.595001, Test acc: 0.579831\n",
      "Epoch: 75, Loss: 0.851922, Train acc: 0.588543, Test acc: 0.597455\n",
      "Epoch: 80, Loss: 0.829131, Train acc: 0.595331, Test acc: 0.605022\n",
      "Epoch: 85, Loss: 0.832442, Train acc: 0.599372, Test acc: 0.613433\n",
      "Epoch: 90, Loss: 0.825642, Train acc: 0.598146, Test acc: 0.612957\n",
      "Epoch: 95, Loss: 0.850544, Train acc: 0.591436, Test acc: 0.614293\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'148.77922 sec'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout1 , dropout2 = 0.15,0.4\n",
    "\n",
    "# bulid a neural network with two hidden layer and droup out rate for each hidden layer is 0.15 and 0.4\n",
    "net = gluon.nn.Sequential()\n",
    "net.add(gluon.nn.Dense(64, activation = 'relu'),\n",
    "        gluon.nn.Dropout(dropout1),\n",
    "        gluon.nn.Dense(16, activation = 'relu'),\n",
    "        gluon.nn.Dropout(dropout2),\n",
    "        gluon.nn.Dense(3)) # 3 labels\n",
    "\n",
    "net.initialize(init.Normal(sigma=0.01))\n",
    "# Loss function is cross entropy of softmax\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate':.001})\n",
    "\n",
    "# Train the module, the result is not good\n",
    "batch_size = 512\n",
    "num_epoch = 100\n",
    "timer = Timer()\n",
    "train(net, train_iter, test_iter, num_epoch, batch_size)\n",
    "'%.5f sec' % timer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-fold validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the whole dataset to be the training data\n",
    "# test data is a segment of training data\n",
    "train_features_data = dummyData.drop(columns ='Severity')\n",
    "train_labels_data = dummyData['Severity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_fold_data(k, i, features, labels):\n",
    "    fold_size = features.shape[0]//k\n",
    "    f_train , l_train = None, None\n",
    "    f_test, l_test = None, None\n",
    "    # iterate each data segment\n",
    "    for j in range(k):\n",
    "        index = slice(j*fold_size, (j+1)*fold_size)\n",
    "        f_part = features[index]\n",
    "        l_part = labels[index]\n",
    "        # i-th data segment will be the test data in i-th iteration of k-fold\n",
    "        if j == i:\n",
    "            f_test = f_part\n",
    "            l_test = l_part\n",
    "        elif f_train is None:\n",
    "            f_train = f_part\n",
    "            l_train = l_part\n",
    "        else:\n",
    "            # other data segment will concated together to be training data\n",
    "            f_train = pd.concat((f_train, f_part), axis = 0)\n",
    "            l_train = pd.concat((l_train, l_part), axis = 0)\n",
    "    \n",
    "    # using ndarray will cause this step as the shape of ndarray will change after extraction of data from ndarray\n",
    "    # so we transfrom the dataset to array after slicing, this question won't happen in np.array\n",
    "    ftrain = nd.array(f_train.values, dtype = 'float32')\n",
    "    ltrain = nd.array(l_train.values, dtype = 'float32')\n",
    "    ftest = nd.array(f_test.values, dtype = 'float32')\n",
    "    ltest = nd.array(l_test.values, dtype = 'float32')\n",
    "    return ftrain, ltrain, ftest, ltest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_k_fold(k, net, train_features, train_labels, num_epoch, batch_size):\n",
    "    for i in range(k):\n",
    "        # extract training data and test data and turn them into data iterator\n",
    "        trainFeatureTemp, trainLabelTemp, testFeatureTemp, testLabelTemp= get_k_fold_data(k,i,train_features, train_labels)\n",
    "        train_iter = d2l.load_array((trainFeatureTemp, trainLabelTemp), batch_size)\n",
    "        test_iter = d2l.load_array((testFeatureTemp, testLabelTemp), batch_size)\n",
    "        # the same training process\n",
    "        for epoch in range(num_epoch):\n",
    "            train_loss = 0.\n",
    "            train_acc = 0.\n",
    "            for data, label in train_iter:\n",
    "                with autograd.record():\n",
    "                    output = net(data)\n",
    "                    loss = softmax_cross_entropy(output, label)\n",
    "                loss.backward()\n",
    "                trainer.step(batch_size)\n",
    "                train_loss += nd.mean(loss).asscalar()\n",
    "                train_acc += accuracy(output, label)\n",
    "            test_acc = evaluate_accuracy(test_iter, net)\n",
    "            # print the result for each 5 epoches\n",
    "            if (epoch+1)% 5 == 0 :\n",
    "                print('K: %d, Epoch: %d, Loss: %f, Train acc: %f, Test acc: %f' % \n",
    "                  (i ,epoch + 1, train_loss/len(train_iter), train_acc/len(train_iter), test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K: 0, Epoch: 5, Loss: 0.942420, Train acc: 0.524151, Test acc: 0.548083\n",
      "K: 0, Epoch: 10, Loss: 0.893661, Train acc: 0.559687, Test acc: 0.578456\n",
      "K: 0, Epoch: 15, Loss: 0.876442, Train acc: 0.570292, Test acc: 0.575463\n",
      "K: 0, Epoch: 20, Loss: 0.865400, Train acc: 0.578780, Test acc: 0.589434\n",
      "K: 0, Epoch: 25, Loss: 0.859876, Train acc: 0.583418, Test acc: 0.582577\n",
      "K: 0, Epoch: 30, Loss: 0.861189, Train acc: 0.584826, Test acc: 0.599368\n",
      "K: 0, Epoch: 35, Loss: 0.863054, Train acc: 0.585131, Test acc: 0.600528\n",
      "K: 0, Epoch: 40, Loss: 0.857032, Train acc: 0.585543, Test acc: 0.590339\n",
      "K: 0, Epoch: 45, Loss: 0.862229, Train acc: 0.584179, Test acc: 0.605824\n",
      "K: 0, Epoch: 50, Loss: 0.848974, Train acc: 0.589441, Test acc: 0.596036\n",
      "K: 1, Epoch: 5, Loss: 0.839138, Train acc: 0.597433, Test acc: 0.615107\n",
      "K: 1, Epoch: 10, Loss: 0.839056, Train acc: 0.600026, Test acc: 0.592458\n",
      "K: 1, Epoch: 15, Loss: 0.832848, Train acc: 0.600591, Test acc: 0.617149\n",
      "K: 1, Epoch: 20, Loss: 0.835314, Train acc: 0.601345, Test acc: 0.610634\n",
      "K: 1, Epoch: 25, Loss: 0.834231, Train acc: 0.601403, Test acc: 0.599225\n",
      "K: 1, Epoch: 30, Loss: 0.833335, Train acc: 0.604084, Test acc: 0.609009\n",
      "K: 1, Epoch: 35, Loss: 0.835464, Train acc: 0.601501, Test acc: 0.619384\n",
      "K: 1, Epoch: 40, Loss: 0.827390, Train acc: 0.607007, Test acc: 0.598962\n",
      "K: 1, Epoch: 45, Loss: 0.833208, Train acc: 0.600284, Test acc: 0.606547\n",
      "K: 1, Epoch: 50, Loss: 0.834621, Train acc: 0.600443, Test acc: 0.609298\n",
      "K: 2, Epoch: 5, Loss: 0.833808, Train acc: 0.601733, Test acc: 0.591804\n",
      "K: 2, Epoch: 10, Loss: 0.832345, Train acc: 0.604922, Test acc: 0.621758\n",
      "K: 2, Epoch: 15, Loss: 0.831558, Train acc: 0.602732, Test acc: 0.622731\n",
      "K: 2, Epoch: 20, Loss: 0.830740, Train acc: 0.604126, Test acc: 0.622202\n",
      "K: 2, Epoch: 25, Loss: 0.835203, Train acc: 0.602615, Test acc: 0.604502\n",
      "K: 2, Epoch: 30, Loss: 0.832909, Train acc: 0.602636, Test acc: 0.590414\n",
      "K: 2, Epoch: 35, Loss: 0.828994, Train acc: 0.602809, Test acc: 0.618125\n",
      "K: 2, Epoch: 40, Loss: 0.833231, Train acc: 0.601690, Test acc: 0.618572\n",
      "K: 2, Epoch: 45, Loss: 0.830700, Train acc: 0.602715, Test acc: 0.617505\n",
      "K: 2, Epoch: 50, Loss: 0.829808, Train acc: 0.602411, Test acc: 0.620943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'214.58635 sec'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout1 , dropout2 = 0.15,0.45\n",
    "net = gluon.nn.Sequential()\n",
    "# we bulid a neural network with two hidden layer.\n",
    "net.add(gluon.nn.Dense(64, activation = 'relu'),\n",
    "        gluon.nn.Dropout(dropout1),\n",
    "        gluon.nn.Dense(16, activation = 'relu'),\n",
    "        gluon.nn.Dropout(dropout2),\n",
    "        gluon.nn.Dense(3))\n",
    "\n",
    "# initialize the parameters of neural net\n",
    "net.initialize(init.Normal(sigma=0.01))\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate':.001})\n",
    "\n",
    "# 3-fold cross validation\n",
    "k = 3\n",
    "num_epoch = 50\n",
    "batch_size = 512\n",
    "timer = Timer()\n",
    "train_k_fold(k,net, train_features_data, train_labels_data, num_epoch, batch_size)\n",
    "'%.5f sec' % timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K: 0, Epoch: 5, Loss: 0.949653, Train acc: 0.525005, Test acc: 0.526515\n",
      "K: 0, Epoch: 10, Loss: 0.900550, Train acc: 0.562258, Test acc: 0.570208\n",
      "K: 0, Epoch: 15, Loss: 0.883537, Train acc: 0.572217, Test acc: 0.569165\n",
      "K: 0, Epoch: 20, Loss: 0.868496, Train acc: 0.582038, Test acc: 0.571536\n",
      "K: 0, Epoch: 25, Loss: 0.858387, Train acc: 0.580471, Test acc: 0.576969\n",
      "K: 0, Epoch: 30, Loss: 0.847056, Train acc: 0.588301, Test acc: 0.604543\n",
      "K: 0, Epoch: 35, Loss: 0.859027, Train acc: 0.584104, Test acc: 0.577208\n",
      "K: 0, Epoch: 40, Loss: 0.859064, Train acc: 0.583046, Test acc: 0.579786\n",
      "K: 0, Epoch: 45, Loss: 0.845339, Train acc: 0.585502, Test acc: 0.611285\n",
      "K: 0, Epoch: 50, Loss: 0.848499, Train acc: 0.589779, Test acc: 0.580684\n",
      "K: 1, Epoch: 5, Loss: 0.849997, Train acc: 0.591321, Test acc: 0.589962\n",
      "K: 1, Epoch: 10, Loss: 0.844705, Train acc: 0.595061, Test acc: 0.614676\n",
      "K: 1, Epoch: 15, Loss: 0.847025, Train acc: 0.597855, Test acc: 0.619027\n",
      "K: 1, Epoch: 20, Loss: 0.844524, Train acc: 0.599210, Test acc: 0.620422\n",
      "K: 1, Epoch: 25, Loss: 0.843565, Train acc: 0.596335, Test acc: 0.615533\n",
      "K: 1, Epoch: 30, Loss: 0.853161, Train acc: 0.594170, Test acc: 0.618976\n",
      "K: 1, Epoch: 35, Loss: 0.848868, Train acc: 0.593151, Test acc: 0.623430\n",
      "K: 1, Epoch: 40, Loss: 0.843404, Train acc: 0.597908, Test acc: 0.617272\n",
      "K: 1, Epoch: 45, Loss: 0.843041, Train acc: 0.600137, Test acc: 0.624131\n",
      "K: 1, Epoch: 50, Loss: 0.843622, Train acc: 0.595661, Test acc: 0.626728\n",
      "K: 2, Epoch: 5, Loss: 0.846435, Train acc: 0.597116, Test acc: 0.622826\n",
      "K: 2, Epoch: 10, Loss: 0.848835, Train acc: 0.594250, Test acc: 0.622394\n",
      "K: 2, Epoch: 15, Loss: 0.841087, Train acc: 0.600160, Test acc: 0.621155\n",
      "K: 2, Epoch: 20, Loss: 0.839382, Train acc: 0.602034, Test acc: 0.623877\n",
      "K: 2, Epoch: 25, Loss: 0.838226, Train acc: 0.605751, Test acc: 0.625715\n",
      "K: 2, Epoch: 30, Loss: 0.839128, Train acc: 0.604300, Test acc: 0.621278\n",
      "K: 2, Epoch: 35, Loss: 0.832368, Train acc: 0.607536, Test acc: 0.625558\n",
      "K: 2, Epoch: 40, Loss: 0.841080, Train acc: 0.607172, Test acc: 0.616028\n",
      "K: 2, Epoch: 45, Loss: 0.837672, Train acc: 0.606681, Test acc: 0.616279\n",
      "K: 2, Epoch: 50, Loss: 0.832783, Train acc: 0.607470, Test acc: 0.618125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'234.15255 sec'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout1 , dropout2 = 0.15,0.45\n",
    "net = gluon.nn.Sequential()\n",
    "# we bulid a neural network with two hidden layer. \n",
    "net.add(gluon.nn.Dense(64, activation = 'relu'),\n",
    "        gluon.nn.Dropout(dropout1),\n",
    "        gluon.nn.Dense(16, activation = 'relu'),\n",
    "        gluon.nn.Dropout(dropout2),\n",
    "        gluon.nn.Dense(3))\n",
    "\n",
    "net.initialize(init.Normal(sigma=0.01))\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate':.001})\n",
    "\n",
    "\n",
    "#train_k_fold(k, net, train_features, train_labels, num_epoch, batch_size)\n",
    "# the result is not good neither\n",
    "k = 3\n",
    "num_epoch = 50\n",
    "batch_size = 512\n",
    "timer = Timer()\n",
    "train_k_fold(k,net, train_features_data, train_labels_data, num_epoch, batch_size)\n",
    "'%.5f sec' % timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using loop to record the data requires too much time\n",
    "# so we collect the hyper parameters and training results manually\n",
    "\n",
    "results = {'layers' : [2,2,2,2,3,3,3,3,3,3,3,3,3],\n",
    "'hidden_neuron':[32,128,128,128,(64,16),(64,16),(64,16),(64,16),(64,16),(64,16),(64,16),(64,16),(64,16)],\n",
    "'first_droupout':[None,None,None,None,0.15,0.15,0.15,0.15,0.15,0.15,0.20, None,0.15],\n",
    "'second_droupout':[None,None,None,None,0.45,0.45,0.45,0.45,0.45,0.45,0.50, None, 0.45],\n",
    "'learning_rate': [.001,.001,.01,.1,.001,.001,.0001,.001,.0001,.0001,.0001,.0001,1],\n",
    "'k' : [4,4,3,3,3,3,3,4,4,4,4,4,3],\n",
    "'num_epoch': [100,50,50,50,50,100,100,100,50,100,100,100,50],\n",
    "'batch_size' : [512,512,512,512,512,512,512,512,512,512,512,512,512],\n",
    "'test_acc' : [0.631811,0.631336,0.617333,0.350696,0.620943,0.622151,0.623949,0.626174,0.609783,0.634133,0.627020,0.632617,0.350419]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layers</th>\n",
       "      <th>hidden_neuron</th>\n",
       "      <th>first_droupout</th>\n",
       "      <th>second_droupout</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>k</th>\n",
       "      <th>num_epoch</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>512</td>\n",
       "      <td>0.631811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>512</td>\n",
       "      <td>0.631336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>512</td>\n",
       "      <td>0.617333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>512</td>\n",
       "      <td>0.350696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>(64, 16)</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>512</td>\n",
       "      <td>0.620943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>(64, 16)</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>512</td>\n",
       "      <td>0.622151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>(64, 16)</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>512</td>\n",
       "      <td>0.623949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>(64, 16)</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>512</td>\n",
       "      <td>0.626174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>(64, 16)</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>512</td>\n",
       "      <td>0.609783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>(64, 16)</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>512</td>\n",
       "      <td>0.634133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>(64, 16)</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>512</td>\n",
       "      <td>0.627020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>(64, 16)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>512</td>\n",
       "      <td>0.632617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>(64, 16)</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>512</td>\n",
       "      <td>0.350419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    layers hidden_neuron  first_droupout  second_droupout  learning_rate  k  \\\n",
       "0        2            32             NaN              NaN         0.0010  4   \n",
       "1        2           128             NaN              NaN         0.0010  4   \n",
       "2        2           128             NaN              NaN         0.0100  3   \n",
       "3        2           128             NaN              NaN         0.1000  3   \n",
       "4        3      (64, 16)            0.15             0.45         0.0010  3   \n",
       "5        3      (64, 16)            0.15             0.45         0.0010  3   \n",
       "6        3      (64, 16)            0.15             0.45         0.0001  3   \n",
       "7        3      (64, 16)            0.15             0.45         0.0010  4   \n",
       "8        3      (64, 16)            0.15             0.45         0.0001  4   \n",
       "9        3      (64, 16)            0.15             0.45         0.0001  4   \n",
       "10       3      (64, 16)            0.20             0.50         0.0001  4   \n",
       "11       3      (64, 16)             NaN              NaN         0.0001  4   \n",
       "12       3      (64, 16)            0.15             0.45         1.0000  3   \n",
       "\n",
       "    num_epoch  batch_size  test_acc  \n",
       "0         100         512  0.631811  \n",
       "1          50         512  0.631336  \n",
       "2          50         512  0.617333  \n",
       "3          50         512  0.350696  \n",
       "4          50         512  0.620943  \n",
       "5         100         512  0.622151  \n",
       "6         100         512  0.623949  \n",
       "7         100         512  0.626174  \n",
       "8          50         512  0.609783  \n",
       "9         100         512  0.634133  \n",
       "10        100         512  0.627020  \n",
       "11        100         512  0.632617  \n",
       "12         50         512  0.350419  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.DataFrame(results)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
